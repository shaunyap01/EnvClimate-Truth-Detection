import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.preprocessing import StandardScaler
from scipy.sparse import hstack

# Define label mapping for ordinal regression
label_mapping = {
    'false': 0,
    'barely-true': 0.25,
    'half-true': 0.5,
    'mostly-true': 0.75,
    'true': 1
}

# Apply label mapping to the dataset
train_data['label'] = train_data['target'].map(label_mapping)
val_data['label'] = val_data['target'].map(label_mapping)
test_data['label'] = test_data['target'].map(label_mapping)

# Initialize TF-IDF Vectorizer and transform the text data
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['statement_clean'])
X_val_tfidf = tfidf_vectorizer.transform(val_data['statement_clean'])
X_test_tfidf = tfidf_vectorizer.transform(test_data['statement_clean'])

# Standardize numerical features and combine them with TF-IDF features
scaler = StandardScaler(with_mean=False)
X_train = hstack([X_train_tfidf, scaler.fit_transform(train_data[['flesch_kincaid_ease', 'polarity', 'subjectivity']].values)])
X_val = hstack([X_val_tfidf, scaler.transform(val_data[['flesch_kincaid_ease', 'polarity', 'subjectivity']].values)])
X_test = hstack([X_test_tfidf, scaler.transform(test_data[['flesch_kincaid_ease', 'polarity', 'subjectivity']].values)])

# Calculate sample weights to address class imbalance
sample_weights = compute_sample_weight(class_weight='balanced', y=train_data['label'])

# Initialize Random Forest Regressor and set up hyperparameter grid for tuning
rf_model = RandomForestRegressor(random_state=42)
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
grid_search.fit(X_train, train_data['label'], sample_weight=sample_weights)

# Retrieve and display the best parameters and corresponding score
print("Best parameters found: ", grid_search.best_params_)
print("Best score: ", -grid_search.best_score_)

# Train the best model on the entire training dataset
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, train_data['label'], sample_weight=sample_weights)

# Predict on the test set and calculate evaluation metrics
test_predictions = best_rf_model.predict(X_test)
mse = mean_squared_error(test_data['label'], test_predictions)
r2 = r2_score(test_data['label'], test_predictions)

# Display the evaluation metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Function to map continuous predictions back to original labels
def reverse_label_mapping_regression(value):
    if value < 0.125:
        return 'false'
    elif value < 0.375:
        return 'barely-true'
    elif value < 0.625:
        return 'half-true'
    elif value < 0.875:
        return 'mostly-true'
    else:
        return 'true'

# Convert predictions to discrete labels and generate confusion matrix
predicted_labels = np.array([reverse_label_mapping_regression(pred) for pred in test_predictions])
conf_matrix = confusion_matrix(test_data['target'], predicted_labels, labels=['false', 'barely-true', 'half-true', 'mostly-true', 'true'])

# Display the confusion matrix and accuracy
print('Confusion Matrix:')
print(conf_matrix)
accuracy = accuracy_score(test_data['target'], predicted_labels)
print(f'Accuracy: {accuracy}')
